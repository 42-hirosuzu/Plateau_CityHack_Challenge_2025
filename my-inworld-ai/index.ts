import 'dotenv/config';
import express from 'express';
import cors from 'cors';
import { GraphBuilder, GraphTypes, RemoteLLMChatNode, CustomNode, ProcessContext } from "@inworld/runtime/graph";
import { renderJinja } from "@inworld/runtime/primitives/llm";

const app = express();
app.use(cors());
app.use(express.json());

const apiKey = process.env.INWORLD_API_KEY;
if (!apiKey) {
  throw new Error("API key is not set in the .env file.");
}

// ‰øÆÊ≠£ÁÇπ1: „Éó„É≠„É≥„Éó„Éà„Çí„Çà„ÇäÁõ¥Êé•ÁöÑ„ÅßÂ†ÖÁâ¢„Å™„ÇÇ„ÅÆ„Å´‰øÆÊ≠£
const characterPrompt = `
## Role & Goal
You are a Master AI Scene Deconstructor and Meshy Prompt Engineer. Your primary goal is to translate a user's vague, natural language description of a scene into a structured JSON array. Each element in the array will contain a highly optimized, technically correct prompt for a single 3D object, ready to be generated by the Meshy-5 3D AI.

## Overall Workflow
1.  **Scene Deconstruction:** Analyze the user's request to understand the theme, location, and constraints. Deconstruct this abstract scene into a list of 5-7 distinct, essential, single 3D objects.
2.  **Technical Prompt Generation:** For EACH of those objects, generate a technically precise, comma-separated prompt string that strictly follows all of the Meshy-5 Core Rules.

## Meshy Prompting Core Rules (MUST be applied to EVERY generated prompt)
-   **Single Object Only:** Each prompt must be for one object.
-   **Use Concrete, Physical Descriptions:** Specify materials, colors, textures, finishes, shapes (e.g., "brushed aluminum," "matte black powder-coated steel").
-   **Avoid Abstract Words & Non-Physical Effects:** Instead of "beautiful," use "mirror finish with chamfered edges." Do not include effects like smoke or magic particles.
-   **Specify A/T-Pose for Characters:** If the object is a character, add "T-pose" or "A-pose".
-   **No Negative Prompts:** Only describe what IS present.
-   **Format:** Each prompt MUST be a comma-separated string following the structure: '[Main Subject], [Shape/Proportions], [Material/Finish/Color], [Style], [Use/Scale], [Quality Tags], (optional: [Pose])'

## Output Constraints
You MUST respond with ONLY a single, valid JSON array of strings: \`["prompt1", "prompt2", ...]\`. Do not add any other text, explanations, or markdown formatting like \`\`\`. Your entire response must be the raw JSON array itself.

## Example
**User Input:** "I want to create a magical fairytale kingdom in Urayasu, Chiba, but the main castle cannot be taller than 51 meters and the style must be welcoming to families."
**Your Final JSON Output:**
[
    "a magical fairytale castle, welcoming and enchanting proportions, white walls with blue spires and intricate details, fantasy style, for a family-friendly park, under 51 meters tall, highly detailed, photorealistic",
    "an ornate streetlamp, whimsical cast-iron pole with a soft glowing lantern, fantasy style, for a fairytale kingdom pathway, 3 meters tall, detailed",
    "a section of pathway, clean wide cobblestone with charming irregular stones, stone material, for a family-friendly park, clean topology",
    "a park bench, carved wood with fairytale animal motifs, sturdy and inviting look, dark stained wood, for a fairytale kingdom, detailed",
    "a vibrant flowerbed, clusters of oversized fantastical flowers, bioluminescent petals, for a magical park, single object, clean topology"
]
`;

const llm = new RemoteLLMChatNode({
  id: "llm",
  provider: "openai",
  modelName: "gpt-4o-mini",
  textGenerationConfig: {
    maxNewTokens: 2048
  },
});

class AppStateToPromptNode extends CustomNode {
    async process(_ctx: ProcessContext, input: { messages: { role: string; content: string }[] }): Promise<GraphTypes.LLMChatRequest> {
        const renderedPrompt: string = await renderJinja(characterPrompt, {
            transcript: input.messages,
        });
        return new GraphTypes.LLMChatRequest({
            messages: [{ role: "system", content: renderedPrompt }],
        });
    }
}

const appStateToPrompt = new AppStateToPromptNode({ id: "app-state-to-prompt" });

const graph = new GraphBuilder({
    id: 'prompt-generator-graph',
    apiKey,
    enableRemoteConfig: false,
})
    .addNode(appStateToPrompt)
    .addNode(llm)
    .addEdge(appStateToPrompt, llm)
    .setStartNode(appStateToPrompt)
    .setEndNode(llm)
    .build();


app.post('/generate-prompt', async (req, res) => {
    const { keyword } = req.body;
    if (!keyword) {
        return res.status(400).json({ error: 'Keyword is required' });
    }

    try {
        const currentMessages = [{ role: "user", content: keyword }];
        const outputStream = await graph.start({ messages: currentMessages });
        
        let aiResponse = "";
        for await (const result of outputStream) {
            result.processResponse({
                // ‰øÆÊ≠£ÁÇπ2: `response.text` „Çí `response.content` „Å´‰øÆÊ≠£
                Content: (response: any) => {
                    if (response.content) { // `text` „Åß„ÅØ„Å™„Åè `content` „Çí‰ΩøÁî®
                       aiResponse += response.content;
                    }
                },
                default: () => { },
            });
        }

        console.log(`üí¨ Inworld AI Raw Response: ${aiResponse}`);

        // AI„ÅÆÂøúÁ≠î„Åã„ÇâJSONÈÉ®ÂàÜ„Å†„Åë„ÇíÊäΩÂá∫„Åô„ÇãÂá¶ÁêÜ„ÇíËøΩÂä†
        const jsonMatch = aiResponse.match(/\[\s*".*?"\s*\]/s);
        if (jsonMatch) {
            res.json({ prompts: JSON.parse(jsonMatch[0]) });
        } else {
            throw new Error('Valid JSON array not found in AI response.');
        }

    } catch (error) {
        console.error('[Inworld] Error during processing:', error);
        res.status(500).json({ error: 'Failed to generate prompt.' });
    }
});

const PORT = Number(process.env.PORT) || 3002;
app.listen(PORT, '0.0.0.0', () => {
    console.log(`ü§ñ Inworld AI server listening on http://0.0.0.0:${PORT}`);
});